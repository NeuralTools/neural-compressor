:orphan:

:py:mod:`neural_compressor.torch.quantization.config`
=====================================================

.. py:module:: neural_compressor.torch.quantization.config


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.torch.quantization.config.RTNConfig
   neural_compressor.torch.quantization.config.GPTQConfig
   neural_compressor.torch.quantization.config.StaticQuantConfig
   neural_compressor.torch.quantization.config.SmoothQuantConfig
   neural_compressor.torch.quantization.config.FP8QConfig



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.torch.quantization.config.get_default_rtn_config
   neural_compressor.torch.quantization.config.get_default_gptq_config
   neural_compressor.torch.quantization.config.get_default_static_config
   neural_compressor.torch.quantization.config.get_default_sq_config



.. py:class:: RTNConfig(weight_dtype: str = 'int', weight_bits: int = 4, weight_group_size: int = 32, weight_sym: bool = True, act_dtype: str = 'fp32', enable_full_range: bool = False, enable_mse_search: bool = False, group_dim: int = 1, return_int: bool = False, double_quant_dtype: str = 'fp32', double_quant_bits: int = 8, double_quant_sym: bool = True, double_quant_group_size: int = 256, white_list: Optional[List[neural_compressor.common.utils.OP_NAME_OR_MODULE_TYPE]] = DEFAULT_WHITE_LIST)




   Config class for round-to-nearest weight-only quantization.


.. py:function:: get_default_rtn_config() -> RTNConfig

   Generate the default rtn config.

   :returns: the default rtn config.


.. py:class:: GPTQConfig(weight_dtype: str = 'int', weight_bits: int = 4, weight_group_size: int = 32, weight_sym: bool = True, block_size: int = 128, act_dtype: str = 'fp32', group_dim: int = 1, nsamples: int = 128, dataloader_len: int = 10, percdamp: float = 0.01, act_order: bool = False, use_max_length: bool = True, pad_max_length: int = 2048, enable_mse_search: bool = False, device=None, layer_wise: bool = False, return_int: bool = False, double_quant_dtype: str = 'fp32', double_quant_bits: int = 8, double_quant_sym: bool = True, double_quant_group_size: int = 256, white_list: Optional[List[neural_compressor.common.utils.OP_NAME_OR_MODULE_TYPE]] = DEFAULT_WHITE_LIST)




   Config class for GPTQ.

   GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.
   https://arxiv.org/abs/2210.17323


.. py:function:: get_default_gptq_config() -> GPTQConfig

   Generate the default gptq config.

   :returns: the default gptq config.


.. py:class:: StaticQuantConfig(w_dtype: str = 'int8', w_sym: bool = True, w_granularity: str = 'per_channel', w_algo: str = 'minmax', act_dtype: str = 'uint8', act_sym: bool = False, act_granularity: str = 'per_tensor', act_algo: str = 'kl', white_list: Optional[List[neural_compressor.common.utils.OP_NAME_OR_MODULE_TYPE]] = DEFAULT_WHITE_LIST)




   Config class for static quantization.


.. py:function:: get_default_static_config() -> StaticQuantConfig

   Generate the default static quant config.

   :returns: the default static quant config.


.. py:class:: SmoothQuantConfig(w_dtype: str = 'int8', w_sym: bool = True, w_granularity: str = 'per_channel', w_algo: str = 'minmax', act_dtype: str = 'uint8', act_sym: bool = False, act_granularity: str = 'per_tensor', act_algo: str = 'kl', alpha: float = 0.5, folding: bool = False, scale_sharing: bool = False, init_alpha: float = 0.5, alpha_min: float = 0.0, alpha_max: float = 1.0, alpha_step: float = 0.1, shared_criterion: str = 'max', enable_blockwise_loss: bool = False, auto_alpha_args: dict = None, white_list: Optional[List[neural_compressor.common.utils.OP_NAME_OR_MODULE_TYPE]] = DEFAULT_WHITE_LIST)




   Config class for smooth quantization.


.. py:function:: get_default_sq_config() -> SmoothQuantConfig

   Generate the default smoothquant config.

   :returns: the default smoothquant config.


.. py:class:: FP8QConfig(weight_dtype: DTYPE_RANGE = torch.float8_e4m3fn, act_dtype: DTYPE_RANGE = torch.float8_e4m3fn, act_algo: Union[str, List[str]] = 'minmax', approach: Union[str, List[str]] = 'static', device: Union[str, List[str]] = 'hpu', white_list: Optional[List[neural_compressor.common.utils.OP_NAME_OR_MODULE_TYPE]] = DEFAULT_WHITE_LIST)




   Config class for FP8 quantization.


