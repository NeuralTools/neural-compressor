:orphan:

:py:mod:`neural_compressor.onnxrt.algorithms.weight_only.rtn`
=============================================================

.. py:module:: neural_compressor.onnxrt.algorithms.weight_only.rtn


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.onnxrt.algorithms.weight_only.rtn.pad_tensor
   neural_compressor.onnxrt.algorithms.weight_only.rtn.quant_tensor
   neural_compressor.onnxrt.algorithms.weight_only.rtn.qdq_tensor
   neural_compressor.onnxrt.algorithms.weight_only.rtn.rtn_quantize



.. py:function:: pad_tensor(weight, group_size, k_blocks)

   Pad tensor rowi so that it can be is divisible by group_size.

   :param weight: weight
   :type weight: array
   :param group_size: how many elements share one scale/zp
   :type group_size: int
   :param k_blocks: the number of block
   :type k_blocks: int

   :returns: paded weight
   :rtype: weight


.. py:function:: quant_tensor(data, num_bits=4, group_size=32, scheme='asym', dtype='int', ratio=1.0)

   Quantize tensor per group.

   :param data: input weight
   :param num_bits: num_bits. Defaults to 4.
   :type num_bits: int, optional
   :param group_size: how many elements share one scale/zp. Defaults to 4.
   :type group_size: int, optional
   :param scheme: quantization scheme. Defaults to "asym".
   :type scheme: str, optional
   :param dtype: data type. Defaults to "int".
   :type dtype: str, optional
   :param ratio: percentile of clip. Defaults to 1.0.
   :type ratio: float, optional

   :returns: quantized weight
             scale: scale
             zero_point: zero point
   :rtype: output


.. py:function:: qdq_tensor(data, num_bits=4, group_size=32, scheme='asym', dtype='int', ratio=1.0)

   Quant dequant tensor per group.

   :param data: input weight
   :param num_bits: num_bits. Defaults to 4.
   :type num_bits: int, optional
   :param group_size: how many elements share one scale/zp. Defaults to 4.
   :type group_size: int, optional
   :param scheme: quantization scheme. Defaults to "asym".
   :type scheme: str, optional
   :param dtype: data type. Defaults to "int".
   :type dtype: str, optional
   :param ratio: percentile of clip. Defaults to 1.0.
   :type ratio: float, optional

   :returns: quant-dequant weight
   :rtype: output


.. py:function:: rtn_quantize(model: Union[onnx.ModelProto, neural_compressor.onnxrt.utils.onnx_model.ONNXModel, pathlib.Path, str], weight_config: Optional[Dict[tuple, dict]] = {}, num_bits: Optional[int] = 4, group_size: Optional[int] = 32, scheme: Optional[str] = 'asym', ratios: Optional[int] = {}, accuracy_level: Optional[int] = 0, providers: Optional[list] = ['CPUExecutionProvider']) -> onnx.ModelProto

   Quantize the model with round to nearst method.

   :param model: onnx model
   :type model: Union[onnx.ModelProto, ONNXModel, Path, str]
   :param weight_config: quantization config
                         For example,
                         weight_config = {
                             '(fc2, "MatMul")':
                                 {
                                     'weight_dtype': 'int',
                                     'weight_bits': 4,
                                     'weight_group_size': 32,
                                     'weight_sym': True,
                                     'accuracy_level': 0
                                 }
                         }. Defaults to {}.
   :type weight_config: Optional[Dict[tuple, dict]], optional
   :param num_bits: num_bits. Defaults to 4.
   :type num_bits: Optional[int], optional
   :param group_size: how many elements share one scale/zp. Defaults to 32.
   :type group_size: Optional[int], optional
   :param scheme: sym or asym. Defaults to "asym".
   :type scheme: Optional[str], optional
   :param ratios: percentile of clip. Defaults to {}.
   :type ratios: Optional[int], optional
   :param accuracy_level: accuracy level. Support 0 (unset), 1(fp32 compute type of jblas kernel),
                          2 (fp16 compute type of jblas kernel), 3 (bf16 compute type of jblas kernel),
                          4 (int8 compute type of jblas kernel). Defaults to 0.
   :type accuracy_level: Optional[int], optional
   :param providers: providers to use. Defaults to ["CPUExecutionProvider"].
   :type providers: Optional[list], optional

   :returns: quantized ONNXModel
   :rtype: onnx.ModelProto


