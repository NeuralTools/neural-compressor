:orphan:

:py:mod:`neural_compressor.common.base_tuning`
==============================================

.. py:module:: neural_compressor.common.base_tuning


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.Evaluator
   neural_compressor.common.base_tuning.ConfigSet
   neural_compressor.common.base_tuning.Sampler
   neural_compressor.common.base_tuning.SequentialSampler
   neural_compressor.common.base_tuning.ConfigLoader
   neural_compressor.common.base_tuning.TuningConfig
   neural_compressor.common.base_tuning.TuningMonitor



Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.init_tuning



Attributes
~~~~~~~~~~

.. autoapisummary::

   neural_compressor.common.base_tuning.default_sampler


.. py:class:: Evaluator


   Evaluator is a collection of evaluation functions.

   .. rubric:: Examples

   def eval_acc(model):
       ...

   def eval_perf(molde):
       ...

   # Usage
   user_eval_fns1 = eval_acc
   user_eval_fns2 = {"eval_fn": eval_acc}
   user_eval_fns3 = {"eval_fn": eval_acc, "weight": 1.0, "name": "accuracy"}
   user_eval_fns4 = [
       {"eval_fn": eval_acc, "weight": 0.5},
       {"eval_fn": eval_perf, "weight": 0.5, "name": "accuracy"},
       ]




.. py:class:: SequentialSampler(config_source: Sized)




   Samples elements sequentially, always in the same order.

   :param config_source: config set to sample from
   :type config_source: _ConfigSet




.. py:class:: TuningConfig(config_set: Union[neural_compressor.common.base_config.BaseConfig, List[neural_compressor.common.base_config.BaseConfig]] = None, sampler: Sampler = default_sampler, tolerable_loss=0.01, max_trials=100)


   Config for auto tuning pipeline.

   .. rubric:: Examples

   # TODO: to refine it
   from neural_compressor.torch.quantization import TuningConfig
   tune_config = TuningConfig(
       config_set=[config1, config2, ...],
       max_trials=3,
       tolerable_loss=0.01
   )

   # Case 1: Tolerable Loss
   fp32_baseline = 100
   config1_metric, config2_metric, ... = 98, 99, ...

   # Tuning result of case 1:
   # The best tuning config is config2, because config2_metric >= fp32_baseline * (1 - tolerable_loss)

   # Case 2: Maximum Trials
   fp32_baseline = 100
   config1_metric, config2_metric, config3_metric, ... = 98, 98, 97, ...

   # Tuning result of case 2:
   # The best tuning config is config2, because of the following:
   # 1. Not achieving the set goal. (config_metric < fp32_baseline * (1 - tolerable_loss))
   # 2. Reached maximum tuning times.




